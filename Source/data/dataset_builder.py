import numpy as np
from sklearn.preprocessing import MinMaxScaler
from data_fetcher import load_multitimeframe_data
from Source.data._data_config import (
    SYMBOL_LIST, TARGET_INTERVAL, TARGET_COLUMN,
    INTERVAL_MINUTES, WINDOW_MINUTES,
    LABEL_THRESHOLDS, REQUIRED_LENGTH
)

def get_threshold(interval):
    return LABEL_THRESHOLDS.get(interval, 0.01)

def build_lstm_dataset(symbol):
    mtf_data = load_multitimeframe_data(symbol)

    if not mtf_data["stock"] or not mtf_data["index"]:
        print("âŒ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨")
        return None, None

    if TARGET_INTERVAL not in mtf_data["stock"]:
        print(f"âŒ {TARGET_INTERVAL} ë¶„ë´‰ ë°ì´í„°ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return None, None

    target_df = mtf_data["stock"][TARGET_INTERVAL]
    threshold = get_threshold(TARGET_INTERVAL)

    features, labels = [], []
    ref_shape = None

    for i in range(WINDOW_MINUTES, len(target_df) - 1):
        anchor_time = target_df.index[i]
        stack = []

        for interval in INTERVAL_MINUTES.keys():
            win_len = REQUIRED_LENGTH[interval]
            for key in ["stock", "index"]:
                df = mtf_data[key].get(interval)
                if df is None or anchor_time not in df.index:
                    continue
                df_slice = df[df.index <= anchor_time].tail(win_len)
                if len(df_slice) < win_len:
                    pad = np.zeros((win_len - len(df_slice), df.shape[1]))
                    slice_arr = np.vstack([pad, df_slice.values])
                else:
                    slice_arr = df_slice.values
                stack.append(slice_arr)

        if len(stack) != len(INTERVAL_MINUTES) * 2:
            continue

        x = np.concatenate(stack, axis=1)
        if ref_shape is None:
            ref_shape = x.shape[1]
        if x.shape[1] != ref_shape:
            continue

        features.append(x)

        # ë¼ë²¨ë§: ì´ì§„ ë¶„ë¥˜ (ìƒìŠ¹ vs í•˜ë½/ë³´í•©)
        future = target_df[TARGET_COLUMN].iloc[i + 1]
        current = target_df[TARGET_COLUMN].iloc[i]
        change = (future - current) / current
        label = 1 if change > threshold else 0
        labels.append(label)

        # ğŸ¯ ì£¼ì„: í–¥í›„ ìˆ˜ìµë¥  íšŒê·€ ì˜ˆì¸¡ ë¼ë²¨ë§ë„ ê³ ë ¤ ê°€ëŠ¥
        # labels.append(change)

    if not features:
        return None, None

    X = np.stack(features)
    y = np.array(labels)

    # ì •ê·œí™”
    X = np.nan_to_num(X, nan=0.0, posinf=1e10, neginf=-1e10)
    scaler = MinMaxScaler()
    X = X.reshape(-1, X.shape[2])
    X = scaler.fit_transform(X)
    X = X.reshape(-1, WINDOW_MINUTES, ref_shape)

    return X, y

def build_generic_dataset(interval: str):
    global TARGET_INTERVAL
    TARGET_INTERVAL = interval

    X_all, y_all = [], []
    expected_dim = None

    for symbol in SYMBOL_LIST:
        print(f"ğŸ“¡ [{symbol} / {interval}] ë°ì´í„°ì…‹ ìƒì„± ì¤‘...")
        X, y = build_lstm_dataset(symbol)

        if X is None or y is None:
            continue
        if expected_dim is None:
            expected_dim = X.shape[2]
        if X.shape[2] != expected_dim:
            print(f"âš ï¸ {symbol}: ì°¨ì› ë¶ˆì¼ì¹˜ â†’ ê±´ë„ˆëœ€")
            continue

        X_all.append(X)
        y_all.append(y)

    if not X_all:
        print(f"âŒ {interval} ê¸°ì¤€ í•™ìŠµ ê°€ëŠ¥í•œ ë°ì´í„° ì—†ìŒ")
        return None, None

    X = np.concatenate(X_all, axis=0)
    y = np.concatenate(y_all, axis=0)
    print(f"âœ… {interval} ê¸°ì¤€ ì´ ìƒ˜í”Œ ìˆ˜: {X.shape[0]}")
    return X, y

def show_label_distribution(y):
    unique, counts = np.unique(y, return_counts=True)
    for u, c in zip(unique, counts):
        print(f"  ğŸŸ¢ ë¼ë²¨ {u}: {c}ê°œ ({(c / len(y)) * 100:.2f}%)")
